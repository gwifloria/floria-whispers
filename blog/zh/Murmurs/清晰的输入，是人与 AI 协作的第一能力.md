---
draft: true
---
上周末我和朋友在家里办了一场读书分享会。结束后，我在和夏老师闲聊时，她提到一个问题：

> “怎样才能让 AI 变得更聪明？我感觉问它问题多了之后，它老是忘东西，也不会主动帮我总结。”

我其实一开始没料到这个问题——可能我是付费用户，目前上下文的关联体验让我意识不到这个问题。

我跟她说，也许这和人脑很像——  
当信息量太大，我们也会在脑中进行压缩和筛选。共通点是：我们都需要主动复盘， AI 需要我们**明确告诉它：哪些信息应该被保留、哪些需要被总结**。

这让我想起年初读过的一本收纳类书籍。  
作者提到，很多人从小被要求“把房间收拾干净”，却从来没人教过——**到底什么才叫收纳**？

- 是把同类物品堆在一起？
    
- 是清走桌面，让空间“看上去”变整洁？
    
- 还是建立一套可持续维护的整理系统？
    

收纳其实是一门学问，是一整套**方法论与指引**，而不是一句“自己收拾一下”的泛指令。

我觉得——这和使用 AI 是同样的道理。

刚开始用 AI 编程时，我也觉得它“不够聪明”，经常给出偏离需求的结果。但使用久了才发现，问题不在于 AI，而在于我：

> 我给的是“模糊指令”，  
> 却期待它交付“精确结果”。

就像家长一句“把房间收拾干净”，  
每个人对“干净”的理解完全不同。

当我开始：

- 明确步骤
    
- 指定范围
    
- 约束目标
    
- 要求总结 / 复盘
    

AI 的工作效率一下子变得非常高。  
与其把 AI 想象成“天才助手”，不如把它当作**一位认真但需要指导的实习生**——

👉 你给的指令越清晰，它的表现就越好。

这件事也让我想到我最近在工作中遇到的一种很典型的状况。

有些合作伙伴在沟通需求时，仿佛真的把自己当作了“甲方宇宙中心”。  
他们不会说清目标、不会拆解需求、不会给边界条件，只会丢一句模糊的：

> “只是改一下 API。”

等结果出来，却义正辞严地说：

> “怎么这个不做？”

可复盘下来才发现——  

**他根本自己都没有把需求盘清楚，就把问题丢出来了。**

这种沟通方式，很像一部分人使用 AI：

给了一个含糊不清的指令，  
却期望对方 magically 理解一切背景与上下文；

当结果不符合期待时，  
责怪的永远不是自己表达不足

把责任外包给别人，  
是一件既省脑子又充满自我优越感的事情。

而这种行为，比 AI 更“人工智能缺失”。