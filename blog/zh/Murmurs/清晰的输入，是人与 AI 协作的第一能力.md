---
draft: false
---
上周末我和朋友在家里办了一场读书分享会。结束后，我在和夏老师闲聊时，她提到一个问题：

> “怎样才能让 AI 变得更聪明？  
> 问它问题多了之后，它老是忘东西，也不会主动帮我总结。”

我一开始其实没太有共鸣——  
可能因为我是付费用户，目前上下文关联做得相对好，所以这种“失忆感”没有那么明显。

我想了想，跟她说：

也许这和人脑很像。  
当信息量太大时，我们会在脑中进行压缩与筛选；  
而 AI 现在还没办法完全自主做这件事，需要我们**明确告诉它哪些信息要保留、哪些需要被总结**。

换句话说：

👉 不只是“问问题”  
👉 而是要“指挥它如何处理信息”

---

这让我想起年初读过的一本收纳书——  
《让你摆脱混乱的人生整理术》。

书里提到一件让我印象很深的事情：

很多人从小就被要求“把房间收拾干净”，  
但却从来没人教过——

> 到底什么才叫“收纳”？

是把同类物品堆到一起？  
是把东西塞进抽屉里，让桌面“看上去”变整洁？  
还是建立一套可以长期维持的整理系统？

作者强调：

收纳不是一句“自己收拾一下”的泛指令，  
它其实是一整套**结构化方法论**。

---

我后来发现——这和使用 AI 的逻辑，几乎一模一样。

刚开始用 AI 编程的时候，我也觉得它“不够聪明”，  
经常给出偏离需求、甚至南辕北辙的结果。

但使用久了才发现，问题很多时候不在 AI，而在我自己：

我给的是模糊输入，  
却期待它交付精确输出。

当我开始：

- 明确角色与目标
    
- 约束范围与前提
    
- 要求总结与复盘
    
- 指定信息保留方式
    

AI 的工作效率一下子变得非常高。

与其把它当作“自动理解一切的天才助手”，  
不如把它看作一位**认真可靠、但需要指导的实习生**：

👉 你给的指令越清晰，它的表现就越好。

---

这件事，也让我重新看了一些职场沟通的场景。

最近我遇到一个很典型的状况：

有些合作伙伴在沟通需求时，仿佛真的把自己当成了“甲方宇宙中心”。  
他们不会讲清目标、不拆解需求、不给边界条件，只丢下一句模糊的：

> “就是简单改一下 API。”

等结果出来，却义正辞严地问：

> “为什么这个没做？”

但每次复盘下来才发现——

**他从一开始，就没有把需求讲清楚。**

这种沟通方式，很像一部分人使用 AI：

给出一个含糊不清的指令，  
却期望对方能够 _magically_ 理解所有背景与上下文；

当结果不符合预期时，  
第一反应不是补充信息、也不是澄清需求，  
而是把责任外包出去，好像这是一件理所当然的事。

说句实话：

👉 这种行为，比 AI 更“缺少智能”。

---

当然，我并不把自己放在道德制高点。

我也有很多时候表达不完整、  
沟通不清楚、  
以为“对方应该懂”。

人会犯错，  
AI 会犯错，  
沟通本就是反复修正、对齐、再前进的过程。

如果说这段时间我学到了一点什么，大概是：

> 无论是与人协作，还是与 AI 协作——  
> 清晰的输入，是一切效率与理解的前提。

而“清晰表达”  
既不是能力测试  
也不是情绪输出  
而是一种对合作对象的尊重。

附上一张最近看到的网图

![[397738300c4ef9fd0f33567f0815a703.jpg]]