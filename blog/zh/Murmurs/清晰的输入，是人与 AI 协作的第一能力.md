---
draft: false
---
上周末我和朋友在家里办了一场读书分享会。结束后，我在和夏老师闲聊时，她提到一个问题：

> “怎样才能让 AI 变得更聪明？我感觉问它问题多了之后，它老是忘东西，也不会主动帮我总结。”

老实说，我一开始有些意外。  
可能因为我是付费用户，目前用下来上下文关联体验还不错，所以并没有强烈感知到这种“容易忘东西”的困扰。

我跟她说，也许这和人脑很像——  
当信息量太大时，我们也会在脑中进行压缩与取舍。区别只是：

👉 人可以主动复盘  
👉 而 AI 需要我们**明确告诉它：哪些内容应该被保留、哪些需要被总结**

这一点让我想起年初读过的一本收纳书：《让你摆脱混乱的人生整理术》。

作者提到，很多人从小被要求“把房间收拾干净”，但从来没人真正教过——

> 到底什么才叫“收纳”？

- 是把同类物品堆到一起？
    
- 是把东西移到角落，让桌面显得更整洁？
    
- 还是建立一套可持续维护的整理系统？
    

收纳其实是一门学问，是一整套**结构与方法论**，而不是一句“自己收拾一下”的泛指令。

我渐渐觉得，这和使用 AI 是同样的道理。

刚开始用 AI 编程时，我也会觉得它“不够聪明”，经常给出偏离预期的结果。但后来我发现，问题往往不在 AI，而在于我：

> 我给的是“模糊指令”，  
> 却期待它交付“精确结果”。

就像家长一句“把房间收拾干净”，  
每个人对“干净”的理解都不一样。

当我开始：

- 说明上下文
    
- 指定范围与边界
    
- 约束目标
    
- 要求复盘与总结
    

AI 的表现立刻变得好很多。  
与其把 AI 想象成“天才助手”，不如把它当作**一位认真、可培养、但需要清晰指引的实习生**——

👉 指令越清晰，它就越高效。

这件事也让我联想到最近的一些工作经历。

有些合作伙伴在沟通需求时，像是默认“别人自然应该懂”。他们不拆解需求、不讲清边界，只丢下一句：

> “只是改一下 API。”

等结果出来，又追问：

> “那这个为什么没做？”

但复盘之后才会发现——  
**他们自己其实也没有先把需求想清楚。**

这种沟通方式，很像一部分人使用 AI——

- 输入含糊的指令
    
- 期待对方自动理解全部背景
    
- 当结果偏离预期
    
- 问题就被归结为“执行不对”
    

有时，并不是工具不够好、也不是对方不努力，  
而是“输入质量”本身就不清晰。

不过，写到这里我也想承认一点：

👉 没有谁能一次性把上下文讲清楚  
👉 我自己也常常表达得不够完整

人会犯错，AI 也会犯错。  
重要的不是“永远不出错”，而是：

- 复盘
    
- 调整结构
    
- 下次说得更清楚
    

而这，或许正是人与 AI 协作中最值得训练的一种能力。